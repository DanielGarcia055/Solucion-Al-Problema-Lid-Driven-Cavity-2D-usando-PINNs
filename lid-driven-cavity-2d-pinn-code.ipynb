{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nimport scipy.optimize\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom matplotlib.gridspec import GridSpec\nfrom tqdm import tqdm\nimport pandas as pd\nimport random\nfrom IPython.display import clear_output\nimport time\nimport dill\ntry:\n    from pyDOE import lhs\nexcept:\n    !pip install pyDOE\n    from pyDOE import lhs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.saving import register_keras_serializable\n\nclass neural_net(tf.keras.Sequential):\n    \n    def __init__(self, capas, act, init):\n        super(neural_net, self).__init__()\n\n    #--------------------------------------------------------------------------------------------------------------------------------------------------\n\n    # Inicialización de la clase\n\n        self.acts = {\"swish\": self.swish,\n                     \"mish\": self.mish,\n                     \"ReLU\": self.ReLU}\n\n        self.act = act\n\n        self.init = init\n\n        self.capas = capas\n\n    # Construcción de la red\n\n        self.add(tf.keras.layers.InputLayer(input_shape = (self.capas[0],)))\n        \n        if self.act == \"mish\":\n\n            self.mensaje = \"Entrada escalada a [-1,1] para usar \" + self.act\n            \n            self.add(tf.keras.layers.Lambda(lambda X: 2.0*(X)/(1)-1.0))\n            \n        \n        if self.act == \"swish\":\n\n            self.mensaje = \"Entrada escalada a [0,1] para usar \" + self.act\n            \n            self.add(tf.keras.layers.Lambda(lambda X: X))\n        \n\n        for ancho in self.capas[1:-1]:\n\n            self.add(tf.keras.layers.Dense(ancho, activation = self.acts[self.act], kernel_initializer = self.init))\n\n        self.add(tf.keras.layers.Dense(self.capas[-1], activation = None, kernel_initializer = self.init ))\n\n\n    # Guardar en listas los tamaños de los pesos w y los sesgos b\n\n        self.sizes_w = []\n        self.sizes_b = []\n\n        for i in range(len(self.capas)-1):\n\n            self.sizes_w.append(self.capas[i]*self.capas[i+1])\n            self.sizes_b.append(self.capas[i+1])\n\n\n    #---------------------------------------------------------------------------------------------------------------------------------------------------------\n\n    # Definición de funciones de activación\n    \n    def swish(self,x):\n        return x*tf.math.sigmoid(x)\n\n\n    def mish(self,x):\n        return x*tf.nn.tanh(tf.math.softplus(x))\n\n\n    def ReLU(self,x):\n        return tf.nn.relu(x)\n\n    #--------------------------------------------------------------------------------------------------------------------------------------------------------\n    # Guardar los pesos en y sesgos en una unica lista ordenada donde primero van los pesos y luego los sesgos\n\n    def obtener_pesos(self):\n\n        w = []\n\n        for i in range(1,len(self.capas)):\n\n            pesos = []\n            sesgos = []\n\n            pesos.extend(self.layers[i].get_weights()[0].flatten())\n            sesgos.extend(self.layers[i].get_weights()[1])\n            w.extend(pesos + sesgos)\n\n        w = np.copy(w, order = \"F\")\n\n        return w\n\n    #-------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n    # Establecer pesos capa por capa\n\n    def establecer_pesos(self, w):\n\n        suma_acum = np.cumsum(np.array(self.sizes_b[0:]) + np.array(self.sizes_w[0:]))\n        suma_acum = np.insert(suma_acum, 0 , 0)\n\n        for i in range(1, len(self.capas)):\n\n            pesos_sesgos = self.obtener_pesos()[suma_acum[i-1]:suma_acum[i]]\n            pesos = pesos_sesgos[0:self.sizes_w[i-1]]\n            sesgos = pesos_sesgos[self.sizes_w[i-1]:self.sizes_w[i-1] + self.sizes_b[i-1]]\n\n            pesos = tf.reshape(pesos, [self.capas[i-1], self.capas[i]])\n\n            pesos_sesgos = [pesos, sesgos]\n\n            self.layers[i].set_weights(pesos_sesgos)\n\n\n\n    def set_weights_2(self, w):\n        for i, layer in enumerate(self.layers[1:]):\n            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n            weights = w[start_weights:end_weights]\n            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n            biases = w[end_weights:end_weights + self.sizes_b[i]]\n            weights_biases = [weights, biases]\n            layer.set_weights(weights_biases)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clase para derivar las salidas de la red neuronal\n\nclass gradientes(tf.keras.layers.Layer):\n\n    def __init__(self, modelo):\n\n        self.modelo = modelo\n\n    def der(self, xy):\n\n        xy = tf.convert_to_tensor(xy)\n\n        x, y = [xy[:, i, tf.newaxis] for i in range(xy.shape[-1])]\n\n        with tf.GradientTape(persistent = True) as dd:\n            \n            dd.watch(x)\n            dd.watch(y)\n\n            with tf.GradientTape(persistent = True) as d:\n\n                d.watch(x)\n                d.watch(y)\n\n                u_v_p = self.modelo(tf.concat([x,y], axis=-1))\n\n                u = tf.cast(u_v_p[:, 0, tf.newaxis], dtype=tf.float32)\n                v = tf.cast(u_v_p[:, 1, tf.newaxis], dtype=tf.float32)\n                p = tf.cast(u_v_p[:, 2, tf.newaxis], dtype=tf.float32)\n\n\n            u_x = d.gradient(u, x)\n            u_x = tf.cast(u_x, dtype=tf.float32)\n\n            v_x = d.gradient(v, x)\n            v_x = tf.cast(v_x, dtype=tf.float32)  \n\n            p_x = d.gradient(p, x)\n            p_x = tf.cast(p_x, dtype=tf.float32)\n\n            u_y = d.gradient(u, y)\n            u_y = tf.cast(u_y, dtype=tf.float32)\n\n            v_y = d.gradient(v, y)\n            v_y = tf.cast(v_y, dtype=tf.float32)  \n\n            p_y = d.gradient(p, y)\n            p_y = tf.cast(p_y, dtype=tf.float32)  \n\n            del d\n\n        u_xx = dd.gradient(u_x, x)\n        u_xx = tf.cast(u_xx, dtype=tf.float32)\n        \n        u_yy = dd.gradient(u_y, y)\n        u_yy = tf.cast(u_yy, dtype=tf.float32)\n\n        v_xx = dd.gradient(v_x, x)\n        v_xx = tf.cast(v_xx, dtype=tf.float32)\n        \n        v_yy = dd.gradient(v_y, y)\n        v_yy = tf.cast(v_yy, dtype=tf.float32)\n\n        del dd\n\n        u_grads = u, u_x, u_y, u_xx, u_yy\n        v_grads = v, v_x, v_y, v_xx, v_yy\n        p_grads = p, p_x, p_y\n\n        return u_grads, v_grads, p_grads","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class training:\n    \n    def __init__(self, modelo, xyt, xyu, xyd, xyl, xyr):\n\n\n        self.modelo = modelo\n        self.xyt = xyt\n        self.xyu = xyu\n        self.xyd = xyd\n        self.xyl = xyl\n        self.xyr = xyr\n        self.training_loss = []\n        self.valida_loss = []\n        self.save_iter = []\n        self.Atest = []\n        self.Btest = []\n        self.Ctest = []\n        self.Dtest = []\n        self.Etest = []\n        self.save_iter_2 = []\n        self.final_time = []\n        self.inicial_iter = 0\n        self.save_time = []\n#--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n    def loss(self):\n        loss = self.modelo.per(self.xyt, self.xyu, self.xyd, self.xyl, self.xyr)\n        return loss\n\n\n    def perdida_y_grads(self, w):\n\n        with tf.GradientTape() as d: \n\n            self.modelo.set_weights_2(w)\n\n            loss_value = self.loss()\n\n        grads = d.gradient(loss_value, self.modelo.trainable_variables)\n\n        grad_flat = []\n\n        for g in grads:\n\n            grad_flat.append(tf.reshape(g, [-1]))\n\n        grad_flat = tf.concat(grad_flat, 0)\n\n        loss_value = np.copy(loss_value, order='F')\n        grad_flat = np.copy(grad_flat, order='F')\n\n        return loss_value, grad_flat\n\n\n    #--------------------------------------------------------------------------------------------------------------------------------------------------\n    @tf.function\n    def adam_train_step(self, optimizer = tf.keras.optimizers.Adam()):\n\n        with tf.GradientTape() as tape:\n\n            perdida = self.loss()\n\n        grads = tape.gradient(perdida, self.modelo.trainable_variables)\n\n        optimizer.apply_gradients(zip(grads, self.modelo.trainable_variables))\n\n        return perdida\n    \n\n    def train(self, iteraciones_adam, iteraciones_LBFGS_max, k=1):\n        \n        \n        adam_hist = []\n\n        if (iteraciones_adam):\n\n            print(\"~~ Optimización Adam ~~\")\n\n            start_time = time.time()\n\n            iteration_start_time = start_time\n\n            for i in range(iteraciones_adam):\n\n                current_loss = self.adam_train_step()\n\n                adam_hist.append(current_loss)\n\n                iteration_time = str(time.time() - iteration_start_time)[:5]\n            \n                print(f'\\rLoss: {current_loss} ; time: {iteration_time} ; iter: {i+1} / {iteraciones_adam}', end='')\n\n                iteration_start_time = time.time()\n\n            tiempo_total = (time.time() - start_time)\n\n            print()\n            print()\n\n            print('Training time: %.4f segundos' % (tiempo_total))\n            \n            self.final_time.append(tiempo_total/60)\n            self.save_time.append(str(tiempo_total)[:5])\n\n    #---------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n        self.LBFGS_hist = [self.loss()]\n\n        if (iteraciones_LBFGS_max):\n            \n            \n            \n            self.xy_interior1 = self.test(101)[0]\n            self.xy_bnd_arriba1 = self.test(101)[1]\n            self.xy_bnd_abajo1 = self.test(101)[2]\n            self.xy_bnd_izquierda1 = self.test(101)[3]\n            self.xy_bnd_derecha1 = self.test(101)[4]\n            \n            self.xy_interior2 = self.test(151)[0]\n            self.xy_bnd_arriba2 = self.test(151)[1]\n            self.xy_bnd_abajo2 = self.test(151)[2]\n            self.xy_bnd_izquierda2 = self.test(151)[3]\n            self.xy_bnd_derecha2 = self.test(151)[4]\n            \n            self.xy_interior3 = self.test(186)[0]\n            self.xy_bnd_arriba3 = self.test(186)[1]\n            self.xy_bnd_abajo3 = self.test(186)[2]\n            self.xy_bnd_izquierda3 = self.test(186)[3]\n            self.xy_bnd_derecha3 = self.test(186)[4]\n            \n            self.xy_interior4 = self.test(221)[0]\n            self.xy_bnd_arriba4 = self.test(221)[1]\n            self.xy_bnd_abajo4 = self.test(221)[2]\n            self.xy_bnd_izquierda4 = self.test(221)[3]\n            self.xy_bnd_derecha4 = self.test(221)[4]\n            \n            self.xy_interior5 = self.test(318)[0]\n            self.xy_bnd_arriba5 = self.test(318)[1]\n            self.xy_bnd_abajo5 = self.test(318)[2]\n            self.xy_bnd_izquierda5 = self.test(318)[3]\n            self.xy_bnd_derecha5 = self.test(318)[4]\n            \n            \n            self.iteraciones_LBFGS_max = iteraciones_LBFGS_max\n\n            maxiter = iteraciones_LBFGS_max - self.inicial_iter\n            \n            self.maxiter = maxiter\n\n            print('~~ Optimización L-BFGS ~~')\n\n            self.iter = self.inicial_iter\n            \n            if k ==  1:\n            \n                self.save_iter.append(self.iter)\n\n                self.save_iter_2.append(self.iter)\n\n\n                self.Atest.append(self.modelo.per(self.xy_interior1, self.xy_bnd_arriba1,\n                                                 self.xy_bnd_abajo1, self.xy_bnd_izquierda1,\n                                                 self.xy_bnd_derecha1).numpy())\n\n                self.Btest.append(self.modelo.per(self.xy_interior2, self.xy_bnd_arriba2,\n                                                 self.xy_bnd_abajo2, self.xy_bnd_izquierda2,\n                                                 self.xy_bnd_derecha2).numpy())\n\n                self.Ctest.append(self.modelo.per(self.xy_interior3, self.xy_bnd_arriba3,\n                                                 self.xy_bnd_abajo3, self.xy_bnd_izquierda3,\n                                                 self.xy_bnd_derecha3).numpy())\n\n                self.Dtest.append(self.modelo.per(self.xy_interior4, self.xy_bnd_arriba4,\n                                                 self.xy_bnd_abajo4, self.xy_bnd_izquierda4,\n                                                 self.xy_bnd_derecha4).numpy())\n\n                self.Etest.append(self.modelo.per(self.xy_interior5, self.xy_bnd_arriba5,\n                                                 self.xy_bnd_abajo5, self.xy_bnd_izquierda5,\n                                                 self.xy_bnd_derecha5).numpy())\n\n                self.training_loss.append(self.loss().numpy())  \n\n            inicial_tiempo = time.time()  \n\n            self.t_last_callback = time.time()\n\n            conjunto = [i*100 for i in range(1, 200 + 1)]\n            \n            conjunto_2 = [i*20 for i in range(1,1000 + 1)]\n\n            self.conjunto = conjunto  \n            \n            self.conjunto_2 = conjunto_2\n            \n                \n#---------------------------------------------------------------------------------------------------------------------                \n\n            results = scipy.optimize.minimize(self.perdida_y_grads,\n                            self.modelo.obtener_pesos(),\n                            method='L-BFGS-B',\n                            jac = True,\n                            callback = self.callback,\n                            options = {\"maxiter\" : self.maxiter,\n                                       \"maxfun\" : 100000,\n                                       \"maxcor\" : 50,\n                                       \"maxls\" : 50,\n                                       \"ftol\" : np.finfo(float).eps})\n            \n            w_optimal = results.x\n\n            self.modelo.set_weights_2(w_optimal)\n            \n            tiempo_total_2 = (time.time() - inicial_tiempo)\n            \n            self.final_time.append(tiempo_total_2/60)\n            \n            self.final_time.append(self.final_time[0] + self.final_time[1])\n\n            print()\n            print()\n            print()\n            print()\n\n            print(\"Modelo entrenado\", \"\\n\", \"Final loss = \", self.training_loss[-1], \"; tiempo: \" , str(tiempo_total_2)[:6], \"segundos\")\n\n\n#-----------------------------------------------------------------------------------------------------------------------\n#Función para generar mallas estructuradas uniformes\n\n    def test(self, points):\n\n        x = np.linspace(0, 1, points)\n        y = x.copy()\n\n        X, Y = np.meshgrid(x,y)\n        xy = np.stack([X[1:-1, 1:-1].flatten(),Y[1:-1, 1:-1].flatten()],axis = -1)\n\n        ceros = np.zeros([points,2])\n        unos = np.ones([points,2])\n\n        xy_bnd_arriba = unos.copy()\n        xy_bnd_arriba[:,0] = x\n\n        xy_bnd_abajo = ceros.copy()\n        xy_bnd_abajo[:,0] = x\n\n        xy_bnd_izquierda = ceros.copy()\n        xy_bnd_izquierda[:,1] = x\n        xy_bnd_izquierda = xy_bnd_izquierda[1:-1]\n\n        xy_bnd_derecha = unos\n        xy_bnd_derecha[:,1] = x\n        xy_bnd_derecha = xy_bnd_derecha[1:-1]\n\n        return xy, xy_bnd_arriba, xy_bnd_abajo, xy_bnd_izquierda, xy_bnd_derecha\n#------------------------------------------------------------------------------------------------------------------        \n        \n    def borrar(self,eliminar):\n\n        file_to_delete = eliminar \n\n        working_dir = '/kaggle/working/'\n\n        file_path = os.path.join(working_dir, file_to_delete)\n\n        if os.path.exists(file_path):\n            os.remove(file_path)\n            \n    \n    def escribir_txt(self):\n           \n        mean_list = list((np.array(self.Atest) + np.array(self.Btest) + np.array(self.Ctest) + np.array(self.Dtest) + np.array(self.Etest))/5).copy()\n\n        indice_optimo = mean_list.index(min(mean_list))\n\n        special_case = indice_optimo*20\n\n\n\n        contador_tiempo = (np.sum(np.array(self.save_time).astype(np.float32)[0:special_case])*2-2)/60\n\n\n        ARCHIVO = \"/kaggle/working/\" + str(self.iter) + \"_visualizador_documento.txt\"\n\n        ARCHIVO_ELIMINAR = \"/kaggle/working/\" + str(self.iter - 20) + \"_visualizador_documento.txt\"\n\n        self.borrar(ARCHIVO_ELIMINAR)\n\n        with open(ARCHIVO , \"w\") as archivo: \n\n            archivo.write(\"Iteración actual = \" + str(self.iter) + \"\\n\")\n            archivo.write(\"Modelo = \" + str(self.modelo.capas) + \"\\n\")\n            archivo.write(\"Semilla = \" + str(seed) + \"\\n\")\n            archivo.write(\"MSE de entrenamiento = \" + str(self.training_loss[special_case]) + \"\\n\")\n            archivo.write(\"MSE del conjunto de prueba A = \" + str(self.Atest[indice_optimo]) + \"\\n\")\n            archivo.write(\"MSE del conjunto de prueba B = \" + str(self.Btest[indice_optimo]) + \"\\n\")\n            archivo.write(\"MSE del conjunto de prueba C = \" + str(self.Ctest[indice_optimo]) + \"\\n\")\n            archivo.write(\"MSE del conjunto de prueba D = \" + str(self.Dtest[indice_optimo]) + \"\\n\")\n            archivo.write(\"MSE del conjunto de prueba E = \" + str(self.Etest[indice_optimo]) + \"\\n\")\n            archivo.write(\"MSE promedio de los conjuntos de prueba = \" + str(min(mean_list)) + \"\\n\")\n            archivo.write(\"Número de iteraciones = \" + str(special_case) + \"\\n\")\n            archivo.write(\"Tiempo = \" + str(contador_tiempo) + \"\\n\")\n            archivo.write(\"Better training MSE = \" + str(min(self.training_loss)) + \"\\n\")\n            archivo.write(\"Iter better training MSE = \" + str(self.training_loss.index(min(self.training_loss))) + \"\\n\")\n\n\n    def callback(self, pars):\n\n        t_iteration = str(time.time() - self.t_last_callback)[:5]\n        \n        self.save_time.append(t_iteration)\n\n        self.iter = self.iter + 1\n\n        loss_value = self.loss().numpy()\n\n        print(f\"\\rloss: {loss_value} ; iteracion:  {self.iter} / {self.iteraciones_LBFGS_max} ; tiempo: {t_iteration} segundos\", end = \"\")\n\n        self.training_loss.append(loss_value)\n\n        self.t_last_callback = time.time()\n        \n        self.save_iter.append(self.iter)\n        \n\n#----------------------------------------------------------------------------------------------------------------------------\n        \n        if self.iter in self.conjunto_2:\n            \n            self.save_iter_2.append(self.iter)\n            \n            self.Atest.append(self.modelo.per(self.xy_interior1, self.xy_bnd_arriba1,\n                                             self.xy_bnd_abajo1, self.xy_bnd_izquierda1,\n                                             self.xy_bnd_derecha1).numpy())\n\n            self.Btest.append(self.modelo.per(self.xy_interior2, self.xy_bnd_arriba2,\n                                             self.xy_bnd_abajo2, self.xy_bnd_izquierda2,\n                                             self.xy_bnd_derecha2).numpy())\n\n            self.Ctest.append(self.modelo.per(self.xy_interior3, self.xy_bnd_arriba3,\n                                             self.xy_bnd_abajo3, self.xy_bnd_izquierda3,\n                                             self.xy_bnd_derecha3).numpy())\n\n            self.Dtest.append(self.modelo.per(self.xy_interior4, self.xy_bnd_arriba4,\n                                             self.xy_bnd_abajo4, self.xy_bnd_izquierda4,\n                                             self.xy_bnd_derecha4).numpy())\n            \n            self.Etest.append(self.modelo.per(self.xy_interior5, self.xy_bnd_arriba5,\n                                             self.xy_bnd_abajo5, self.xy_bnd_izquierda5,\n                                             self.xy_bnd_derecha5).numpy())\n            \n            self.escribir_txt()\n            \n            \n#-----------------------------------------------------------------------------------------------------------------------------        \n        \n        if self.iter in self.conjunto:\n            \n            clear_output()\n            \n            modelo_guardado = '/kaggle/working/'  + \"pesos_guardados\"\n\n            self.modelo.save_weights( modelo_guardado + \".weights.h5\")\n\n            print(f\" ------> Punto de guardado: {int(self.iter/100)} / {int(self.iteraciones_LBFGS_max/100)} ;  loss guardada = {loss_value}\", end = \"\")\n            \n            with open('/kaggle/working/training_loss.pkl', 'wb') as f1:\n                dill.dump(self.training_loss, f1)\n                \n            with open('/kaggle/working/Atest_loss.pkl', 'wb') as f3:\n                dill.dump(self.Atest, f3)\n            \n            with open('/kaggle/working/Btest_loss.pkl', 'wb') as f4:\n                dill.dump(self.Btest, f4)\n                \n            with open('/kaggle/working/Ctest_loss.pkl', 'wb') as f5:\n                dill.dump(self.Ctest, f5)\n            \n            with open('/kaggle/working/Dtest_loss.pkl', 'wb') as f6:\n                dill.dump(self.Dtest, f6)\n                \n            with open('/kaggle/working/Etest_loss.pkl', 'wb') as f7:\n                dill.dump(self.Etest, f7)\n                \n            with open('/kaggle/working/save_iter.pkl', 'wb') as f8:\n                dill.dump(self.save_iter, f8)\n                \n            with open('/kaggle/working/save_iter2.pkl', 'wb') as f9:\n                dill.dump(self.save_iter_2, f9)\n                \n            with open('/kaggle/working/save_time.pkl', 'wb') as f10:\n                dill.dump(self.save_time, f10)\n                \n            with open('/kaggle/working/inicial_iter.pkl', 'wb') as f11:\n                dill.dump(self.inicial_iter, f11)\n            \n            \n#------------------------------------------------------------------------------------------------------------------------            \n        \n            # Actualizar los datos de las líneas de la gráfica\n            \n            plt.ion()\n            self.fig, self.ax = plt.subplots()\n            self.training_line, = self.ax.plot([], [], label=\"MSE de entrenamiento\")\n            self.Aline, = self.ax.plot([], [], label=\"MSE del conjunto de prueba A\", linestyle = \"--\")\n            self.Bline, = self.ax.plot([], [], label=\"MSE del conjunto de prueba B\", linestyle = \"dashdot\")\n            self.Cline, = self.ax.plot([], [], label=\"MSE del conjunto de prueba C\", linestyle = \"dotted\")\n            self.Dline, = self.ax.plot([], [], label=\"MSE del conjunto de prueba D\")\n            self.Eline, = self.ax.plot([], [], label=\"MSE del conjunto de prueba E\", color = \"yellow\")\n\n            self.fig.suptitle(\"MSE de entrenamiento y de los conjuntos de prueba en el modelo\\n\" + str(self.modelo.capas)\n                              + \" con función de activación \" + str(self.modelo.act), x=0.5, ha='center')\n            self.ax.legend()\n            plt.xlabel(\"Número de iteraciones L-BFGS-B\")\n            plt.ylabel(\"Error cuadrático medio\")\n            plt.gca().annotate('Total iteraciones = ' + str(self.iter), \n                               xy=(0.6, 0.55), xycoords='axes fraction', fontsize=9, color='black')\n            plt.tight_layout()        \n        \n            \n            self.training_line.set_data(self.save_iter, self.training_loss)\n            \n            self.Aline.set_data(self.save_iter_2, self.Atest)\n            self.Bline.set_data(self.save_iter_2, self.Btest)\n            self.Cline.set_data(self.save_iter_2, self.Ctest)\n            self.Dline.set_data(self.save_iter_2, self.Dtest)\n            self.Eline.set_data(self.save_iter_2, self.Etest)\n            \n            \n            # Actualizar los límites de los ejes\n            self.ax.relim()\n            self.ax.autoscale_view()\n\n            \n            self.fig.canvas.draw()\n            plt.pause(0.01)  # Esto es necesario para que la gráfica se actualice en tiempo real\n            \n            self.modelo.capas\n            \n            global nb, nf\n            \n            nombre_archivo = '/kaggle/working/'  +  str(self.modelo.capas)[1:-1] + \"_\" + str(self.modelo.act) + \"_\" + str(self.modelo.init) + \"_\" + str(nb) + \"_\" + str(nf)\n        \n            self.fig.savefig(nombre_archivo + \".png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class navier_stokes(neural_net):\n    \n#rho es la densidad y mu la viscosidad dinámica\n\n    def __init__(self, net, rho=1, mu=0.01):\n        super(navier_stokes, self).__init__(net.capas, net.act, net.init)\n\n\n        self.net = net\n        self.rho = rho\n        self.mu = mu\n        \n        if red.act == \"mish\":\n            print(red.mensaje)\n            \n        if red.act == \"swish\":\n            print(red.mensaje)\n\n\n    def deri(self, xy_points):\n        return gradientes(pinn).der(xy_points)\n\n\n    def per(self, xy_eqn, xy_up_bnd, xy_down_bnd, xy_left_bnd, xy_right_bnd):\n\n# EDPs de navier stokes --------------------------------------------------------------------------------------------\n\n        u_grads, v_grads, p_grads = self.deri(xy_eqn)\n\n        u, u_x, u_y, u_xx, u_yy = u_grads\n\n        v, v_x, v_y, v_xx, v_yy = v_grads\n\n        p, p_x, p_y = p_grads\n\n\n\n        r_u = u*u_x + v*u_y + p_x/self.rho - self.mu*(u_xx + u_yy) \n        r_v = u*v_x + v*v_y + p_y/self.rho - self.mu*(v_xx + v_yy)\n        r_div = u_x + v_y\n\n\n# Condiciones de contorno ------------------------------------------------------------------------------------------\n\n        u_up, v_up = pinn(xy_up_bnd)[:,0, tf.newaxis], pinn(xy_up_bnd)[:,1, tf.newaxis]\n\n        u_down, v_down = pinn(xy_down_bnd)[:,0, tf.newaxis], pinn(xy_down_bnd)[:,1, tf.newaxis]\n\n        u_left, v_left = pinn(xy_left_bnd)[:,0, tf.newaxis], pinn(xy_left_bnd)[:,1, tf.newaxis]\n\n        u_right, v_right = pinn(xy_right_bnd)[:,0, tf.newaxis], pinn(xy_right_bnd)[:,1, tf.newaxis]\n    \n# Residual de las condiciones de contorno --------------------------------------------------------------------------------------------\n\n        yB = tf.reduce_mean(tf.square(u_up - 1.0)) + tf.reduce_mean(tf.square(v_up)) + \\\n        tf.reduce_mean(tf.square(u_down)) + tf.reduce_mean(tf.square(v_down)) + \\\n        tf.reduce_mean(tf.square(u_left)) + tf.reduce_mean(tf.square(v_left)) + \\\n        tf.reduce_mean(tf.square(u_right)) + tf.reduce_mean(tf.square(v_right))\n    \n#Residual de las EDPs --------------------------------------------------------------------------\n\n        yS = tf.reduce_mean(tf.square(r_u)) + tf.reduce_mean(tf.square(r_v)) + \\\n        tf.reduce_mean(tf.square(r_div))\n\n#Función de costo -------------------------------------------------------------------------------------------------\n        y = yS + yB\n#------------------------------------------------------------------------------------------------------------------\n        return y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GENERAR DATOS DE ENTRENAMIENTO**","metadata":{}},{"cell_type":"code","source":"seed = 8\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\nnf = 35000 #Puntos en el interior del dominio\nnb = 500 #Puntos por pared\n\n#Función para generar los datos de entrenamiento\ndef gen_training_data(Nf = nf, Nb = nb):\n\n    #Muestreo con hipercubo latino\n\n    xy_train = lhs(2,Nf)\n    \n    #Arreglos de ceros y unos\n\n    unos = np.ones([Nb,2])\n    ceros = np.zeros([Nb,2])\n\n    #Muestreo aleatorio para las condiciones de contorno\n\n    xy_up = unos.copy()\n    xy_up[:,0] = np.random.rand(Nb)\n\n    xy_down = ceros.copy()\n    xy_down[:,0] = np.random.rand(Nb)\n\n    xy_left = ceros.copy()\n    xy_left[:,1] = np.random.rand(Nb)\n\n    xy_right = unos.copy()\n    xy_right[:,1] = np.random.rand(Nb)\n    \n    return xy_train ,xy_up, xy_down, xy_left, xy_right\n\n#Datos de entrenamiento\nxy_train = gen_training_data()[0]\nxy_up = gen_training_data()[1]\nxy_down = gen_training_data()[2]\nxy_left = gen_training_data()[3]\nxy_right = gen_training_data()[4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 11\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\n\nMODELO = [2, 130, 3]\n\nred = neural_net(MODELO, act = \"swish\",  init = \"he_normal\")\n\npinn = navier_stokes(red)\n\nentrenamiento = training(pinn, xy_train ,xy_up, xy_down, xy_left, xy_right)\n\nentrenamiento.train(1500, 20000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}